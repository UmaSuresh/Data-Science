getwd()
list.files()
names(pf)
names(test1)
qplot(x=dob_day,data=pf)
ggplot(test1)
install.packages('ggplot2')
library(ggplot2)
qplot(x=test1$V1,data=test1)
test2 <- read.csv("~/R-Software/test2.csv", header=FALSE)
View(test2)
qplot(x=test2$V1,data=test2)
qplot(x=test2$V1,data=test2$V2)
qplot(x=test2$V2,data=test2)
qplot(x=test2$V2,data=test2&V1)
qplot(x=test2$V2,data=test2$V1)
qplot(x=test2$V2,data=test2$V2)
qplot(x=test2)
qplot(x=test2$V2,data=test2)
ggplot(x=dob_day,data=pf)
install.packages(ggplot2)
install.packages("ggplot2")
load("~/R-Software/.RData")
pseudo_facebook <- read.delim("~/R-Software/pseudo_facebook.tsv")
View(pseudo_facebook)
pf<-read.csv(pseudo_facebook.csv)
pf<-read.csv(pseudo_facebook)
pf<-read.tsv(pseudo_facebook.tsv)
View(pseudo_facebook)
pf<-read.tsv(pseudo_facebook)
pf<-read.csv(pseudo_facebook)
ggplot(x=test2$V1,y=test2$V2)
help.search("geom_point")
ggplot(0)
pf<-read.csv('pseudo_facebook.tsv',sep='/')
ggetwd()
?ggplot
ggplot()
install.packages('ggplot2')
install.packages("ggplot2")
pkgs <- c('reshape2', 'plyr', 'ggplot2', 'dplyr', 'data-table', 'lahman')
install.packages(pkgs)
pew <- read.delim(
file = "http://stat405.had.co.nz/data/pew.txt",
header = TRUE,
stringsAsFactors = FALSE,
check.names = F
)
head(pew)
tb <- read.csv(
file = "http://stat405.had.co.nz/data/tb.csv",
header = TRUE,
stringsAsFactors = FALSE
)
head(tb)
View(pew)
View(tb)
library(reshape2)
pew_tidy <- melt(
data = pew,
id = "religion",
variable.name = "income",
value.name = "frequency"
)
View(pew_tidy)
View(pew)
pew_tidy <- melt(
data = pew,
id = "religion",
variable.name = "income",
value.name = "uency"
)
pew_tidy <- melt(
data = pew,
id = "religion",
variable.name = "income",
value.name = "frequency"
)
tb <- read.csv(
file = "data/tb.csv",
header = TRUE,
stringsAsFactors = FALSE
)
pew <- read.delim(
file = "data/pew.txt",
header = TRUE,
stringsAsFactors = FALSE,
check.names = F
)
View(pseudo_facebook)
setwd("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression")
getwd()
NBA = read.csv(“C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression/NBA_train.csv”)
str(NBA)
NBA <- read.csv(“C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression/NBA_train.csv”)
str(NBA)
NBA <- read.csv(“C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression/NBA_train.csv”)
NBA <- read.csv("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.csv”)
NBA <- read.csv("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.csv”)
NBA <-  read.delim("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.tsv")
View(NBA)
NBA <-  read.delim("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.csv")
View(NBA)
NBA <-  read.delim("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.csv")
View(NBA)
View(NBA)
View(NBA)
View(NBA)
NBA <-  read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.csv")
View(NBA)
view(NBA)
str(NBA)
table(NBA$W, NBA$Playoffs)
# Compute Points Difference
NBA$PTSdiff <- NBA$PTS - NBA$oppPTS
# Compute Points Difference
NBA$PTSdiff <- NBA$PTS - NBA$oppPTS
# Check for linear relationship
plot(NBA$PTSdiff, NBA$W)
# Linear regression model for wins
WinsReg = lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
# Linear regression model for points scored
PointsReg = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + BLK + TOV + STL, data=NBA)
summary(PointsReg)
# Sum of Squared Errors
PointsReg$residuals
SSE = sum(PointsReg$residuals^2)
SSE
# Root mean squared error
RMSE = sqrt(SSE/nrow(NBA))
RMSE
# Remove insignifcant variables
PointsReg3 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data=NBA)
summary(PointsReg3)
# Compute SSE and RMSE again
SSE = sum(PointsReg3$residuals^2)
SSE
RMSE = sqrt(SSE/nrow(NBA))
RMSE
# Check for correlations
RegVar <-  NBA[c("X2PA", "X3PA", "FTA", "AST", "ORB", "STL")]
cor(RegVar)
# Read in test set
NBA_test <- read.csv("NBA_test.csv")
# Make predictions on test set
PointsPredictions = predict(PointsReg, newdata=NBA_test)
# Compute out-of-sample R^2
SSE = sum((PointsPredictions - NBA_test$PTS)^2)
SST = sum((mean(NBA$PTS) - NBA_test$PTS)^2)
R2 = 1 - SSE/SST
R2
# Compute the RMSE
RMSE = sqrt(SSE/nrow(NBA_test))
RMSE
# Read in test set
NBA_test <- read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_test.csv")
# Read in test set
NBA_test <- read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_test.csv")
# Make predictions on test set
PointsPredictions = predict(PointsReg, newdata=NBA_test)
# Compute out-of-sample R^2
SSE = sum((PointsPredictions - NBA_test$PTS)^2)
SST = sum((mean(NBA$PTS) - NBA_test$PTS)^2)
R2 = 1 - SSE/SST
R2
# Compute the RMSE
RMSE = sqrt(SSE/nrow(NBA_test))
RMSE
View(NBA_test)
View(NBA)
statedata <-  read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/statedata.csv")
data(state)
statedata <- cbind(data.frame(state.x77), state.abb, state.area, state.center,  state.division, state.name, state.region)
View(NBA)
setwd("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression")
getwd()
## List of packages
# Read in the data
NBA <-  read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.csv")
str(NBA)
# How many wins to make the playoffs?
table(NBA$W, NBA$Playoffs)
# Compute Points Difference
NBA$PTSdiff <- NBA$PTS - NBA$oppPTS
# Check for linear relationship
plot(NBA$PTSdiff, NBA$W)
# Linear regression model for wins
WinsReg = lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
# Linear regression model for points scored
PointsReg = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + BLK + TOV + STL, data=NBA)
summary(PointsReg)
# Sum of Squared Errors
PointsReg$residuals
SSE = sum(PointsReg$residuals^2)
SSE
# Root mean squared error
RMSE = sqrt(SSE/nrow(NBA))
RMSE
# Remove insignifcant variables
PointsReg3 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data=NBA)
summary(PointsReg3)
# Compute SSE and RMSE again
SSE = sum(PointsReg3$residuals^2)
SSE
RMSE = sqrt(SSE/nrow(NBA))
RMSE
# Check for correlations
RegVar <-  NBA[c("X2PA", "X3PA", "FTA", "AST", "ORB", "STL")]
cor(RegVar)
# Read in test set
NBA_test <- read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_test.csv")
# Make predictions on test set
PointsPredictions = predict(PointsReg, newdata=NBA_test)
# Compute out-of-sample R^2
SSE = sum((PointsPredictions - NBA_test$PTS)^2)
SST = sum((mean(NBA$PTS) - NBA_test$PTS)^2)
R2 = 1 - SSE/SST
R2
# Compute the RMSE
RMSE = sqrt(SSE/nrow(NBA_test))
RMSE
etwd("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression")
getwd()
## List of packages
# Read in the data
statedata <-  read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/statedata.csv")
data(state)
#PROBLEM 1.1 - DATA EXPLORATION (1/1 point)
#We begin by exploring the data by examining the latitude and longitude of each state. Plot all of the states' centers with latitude on the y axis (the "y" variable in our dataset) and longitude on the x axis (the "x" variable in our dataset). The shape of the plot should be the familiar outline of the United States! Note that Alaska and Hawaii have had their coordinates adjusted to appear just off of the west coast.
#In the R command you used to generate this plot, which variable name did you use as the first argument?
plot(statedata$x, statedata$y)
#statedata$x
#Using the tapply command, determine which region of the US (West, North Central, South, or Northeast) has the highest average high school graduation rate of all the states in the region:
tapply(statedata$HS.Grad, statedata$state.region, mean, na.rm=T)
#West
#Now, make a boxplot of the murder rate by region (for more information about creating boxplots in R, type ?boxplot in your console).
#Which region has the highest median murder rate?
boxplot(statedata$Murder ~ statedata$state.region)
#South
#You should see that there is an outlier in the Northeast region of the boxplot you just generated. Which state does this correspond to? (Hint: There are many ways to find the answer to this question, but one way is to use the subset command to only look at the Northeast data.)
NE<- subset(statedata, state.region == "Northeast")
NE$state.name[which.max(NE$Murder)]
#New York
names(statedata)
#We would like to build a model to predict life expectancy by state using the state statistics we have in our dataset.
#Build the model with all potential variables included (Population, Income, Illiteracy, Murder, HS.Grad, Frost, and Area). Note that you should use the variable "Area" in your model, NOT the variable "state.area".
#What is coefficient for income?
lm1<- lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost + Area, data = statedata)
summary(lm1)
#-2.180e-05
#Call the coefficient for income x (the answer to Problem 2.1). What is the interpretation of the coefficient x?
#For a one unit increase in income, predicted life expectancy decreases by |x|
summary(lm2)
#Now plot a graph of life expectancy vs. income using the command:
#plot(statedata$Income, statedata$Life.Exp)
#Visually observe the plot. What appears to be the relationship?
plot(statedata$Income, statedata$Life.Exp)
#Life expectancy is somewhat positively correlated with income. Status: correct
#The model we built does not display the relationship we saw from the plot of life expectancy vs. income. Which of the following explanations seems the most reasonable?
#Multicollinearity
#Recall that we discussed the principle of simplicity: that is, a model with fewer variables is preferable to a model with many unnnecessary variables. Experiment with removing independent variables from the original model. Remember to use the significance of the coefficients to decide which variables to remove (remove the one with the largest "p-value" first, or the one with the "t value" closest to zero), and to remove them one at a time (this is called "backwards variable selection"). This is important due to multicollinearity issues - removing one insignificant variable may make another previously insignificant variable become significant.
#You should be able to find a good model with only 4 independent variables, instead of the original 7. Which variables does this model contain?
lm2<- lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = statedata)
#Population, Murder, Frost, HS.Grad
#Removing insignificant variables changes the Multiple R-squared value of the model. By looking at the summary output for both the initial model (all independent variables) and the simplified model (only 4 independent variables) and using what you learned in class, which of the following correctly explains the change in the Multiple R-squared value?
#We expect the "Multiple R-squared" value of the simplified model to be slightly worse than that of the initial model. It can't be better than the "Multiple R-squared" value of the initial model. Status: correct
#Using the simplified 4 variable model that we created, we'll now take a look at how our predictions compare to the actual values.
#Take a look at the vector of predictions by using the predict function (since we are just looking at predictions on the training set, you don't need to pass a "newdata" argument to the predict function).
#Which state do we predict to have the lowest life expectancy? (Hint: use the sort function)
sort(predict(lm2, statedata))
#Alabama
statedata$state.name[which.min(statedata$Life.Exp)]
#South Carolina
#Which state do we predict to have the highest life expectancy?
sort(predict(lm2, statedata))
#Washington
statedata$state.name[which.max(statedata$Life.Exp)]
#Hawaii
#Take a look at the vector of residuals (the difference between the predicted and actual values).
#For which state do we make the smallest absolute error?
pred.state<- predict(lm2, statedata)
sort(abs(lm2$residuals))
#Indiana
#For which state do we make the largest absolute error?
#Hawaii
# Unit 3, Modeling the Expert
setwd("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression")
getwd()
# Video 4
# Read in dataset
quality = read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/quality.csv")
# Look at structure
str(quality)
# Table outcome
table(quality$PoorCare)
# Baseline accuracy
98/131
# Install and load caTools package
install.packages("caTools")
library(caTools)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# Make predictions on training set
predictTrain = predict(QualityLog, type="response")
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
# Video 5
# Confusion matrix for threshold of 0.5
table(qualityTrain$PoorCare, predictTrain > 0.5)
# Sensitivity and specificity
10/25
70/74
# Confusion matrix for threshold of 0.7
table(qualityTrain$PoorCare, predictTrain > 0.7)
# Sensitivity and specificity
8/25
73/74
# Confusion matrix for threshold of 0.2
table(qualityTrain$PoorCare, predictTrain > 0.2)
# Sensitivity and specificity
16/25
54/74
# Video 6
# Install and load ROCR package
install.packages("ROCR")
library(ROCR)
# Prediction function
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")
# Plot ROC curve
plot(ROCRperf)
# Add colors
plot(ROCRperf, colorize=TRUE)
# Add threshold labels
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
plot(NBA$PTSdiff, NBA$W)
WinsReg = lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
setwd("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression")
getwd()
## List of packages
# Read in the data
NBA <-  read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_train.csv")
str(NBA)
# How many wins to make the playoffs?
table(NBA$W, NBA$Playoffs)
# Compute Points Difference
NBA$PTSdiff <- NBA$PTS - NBA$oppPTS
# Check for linear relationship
plot(NBA$PTSdiff, NBA$W)
# Linear regression model for wins
WinsReg = lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
# Linear regression model for points scored
PointsReg = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + BLK + TOV + STL, data=NBA)
summary(PointsReg)
# Sum of Squared Errors
PointsReg$residuals
SSE = sum(PointsReg$residuals^2)
SSE
# Root mean squared error
RMSE = sqrt(SSE/nrow(NBA))
RMSE
# Remove insignifcant variables
PointsReg3 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data=NBA)
summary(PointsReg3)
# Compute SSE and RMSE again
SSE = sum(PointsReg3$residuals^2)
SSE
RMSE = sqrt(SSE/nrow(NBA))
RMSE
# Check for correlations
RegVar <-  NBA[c("X2PA", "X3PA", "FTA", "AST", "ORB", "STL")]
cor(RegVar)
# Read in test set
NBA_test <- read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/NBA_test.csv")
# Make predictions on test set
PointsPredictions = predict(PointsReg, newdata=NBA_test)
# Compute out-of-sample R^2
SSE = sum((PointsPredictions - NBA_test$PTS)^2)
SST = sum((mean(NBA$PTS) - NBA_test$PTS)^2)
R2 = 1 - SSE/SST
R2
# Compute the RMSE
RMSE = sqrt(SSE/nrow(NBA_test))
RMSE
# Unit 3, Modeling the Expert
setwd("C://Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Linear Regression")
getwd()
# Video 4
# Read in dataset
quality = read.csv("C:/Users/usureshkumar/Documents/R-Software/Dive into EDA -Project files/Data-Science/Data/quality.csv")
# Look at structure
str(quality)
# Table outcome
table(quality$PoorCare)
# Baseline accuracy
98/131
# Install and load caTools package
install.packages("caTools")
library(caTools)
# Randomly split data
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split
# Create training and testing sets
qualityTrain = subset(quality, split == TRUE)
qualityTest = subset(quality, split == FALSE)
# Logistic Regression Model
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data=qualityTrain, family=binomial)
summary(QualityLog)
# Make predictions on training set
predictTrain = predict(QualityLog, type="response")
# Analyze predictions
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
# Video 5
# Confusion matrix for threshold of 0.5
table(qualityTrain$PoorCare, predictTrain > 0.5)
# Sensitivity and specificity
10/25
70/74
# Confusion matrix for threshold of 0.7
table(qualityTrain$PoorCare, predictTrain > 0.7)
# Sensitivity and specificity
8/25
73/74
# Confusion matrix for threshold of 0.2
table(qualityTrain$PoorCare, predictTrain > 0.2)
# Sensitivity and specificity
16/25
54/74
# Video 6
# Install and load ROCR package
install.packages("ROCR")
library(ROCR)
# Prediction function
ROCRpred = prediction(predictTrain, qualityTrain$PoorCare)
# Performance function
ROCRperf = performance(ROCRpred, "tpr", "fpr")
# Plot ROC curve
plot(ROCRperf)
# Add colors
plot(ROCRperf, colorize=TRUE)
# Add threshold labels
plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))
install.packages("caTools")
install.packages("ROCR")
install.packages("ROCR")
